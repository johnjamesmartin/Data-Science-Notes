## 35. Deploying Models to Real-Time Systems

It's all well and good practicing models the likes of a Jupyter notebook, but this runtime is not available in external applications. Ideally, we would like to deploy models to run in a website or app. To do this, we first need to separate our training (computations) and our predictions (generated data served to consumers) by:

1. Training the model periodically offline

2. Pushing the model (or its results) to a _web service_

3. Having the app or website call this web service using something like a RESTful API

### 35.1. Example: Google Cloud ML

- Dump your trained classifier using sklearn.externals:

  `from sklearn.externals import joblib`
  `joblib.dump(clf, 'model.joblib')`

- Upload model.joblib to Google cloud storage, specifying the scikt-learn framework

- Cloud ML Engine exposes a REST API that you can call to make predictions in real-time

### 35.2. Example 2: Amazon AWS (recommender system)

![AWS recommender](https://i.imgur.com/2BVNcVE.png 'AWS recommender')

### 35.3. Other Approaches

- Roll your own web service with Flask or another framework. However this requires you provide and maintain servers

- Go all-in with a platform. There are many machine-learning services available for us to use, such as speech recognition, which the likes of Amazon AWS provides
