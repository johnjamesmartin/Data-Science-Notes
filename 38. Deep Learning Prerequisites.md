## 38. Deep Learning Prerequisites

Deep learning is a subset of machine learning in artificial intelligence (AI) that has networks capable of learning unsupervised from data that is unstructured or unlabeled. It's also known as deep neural learning or deep neural networks.

Before we get started on this topic, however, there are some prerequisites to familiarise ourselves with.

## 38.1. Gradient Descent

Gradient descent is an optimisation algorithm for minimising error over multiple steps in order to find optimal parameters for a given problem.

![Gradient descent](https://i.imgur.com/q5puURo.png 'Gradient descent')

The idea is to pick some position at random on a curve of data points and measure the errors it produces on our system — moving along the curve to different positions until the error minimises itself.

## 38.2. autodiff

Gradient descent requires knowledge of the gradient from your cost function (MSE). Mathematically, we need the first partial derivatives of all the inputs. This is hard and inefficient if you throw calculus at the problem. IThis is where autodiff comes in — it is optimised for many inputs and few outputs (like a neuron) and computes all partial derivatives in # of outputs + 1 graph traversals. This is fundametally a calculus trick and what Tensorflow uses.

## 38.3. softmax

Softmax is used for classification. Given a score for each class it produces a probability of each class. The class with the highest probability is the "answer" you get.

This might be used by a self-driving car that detects and classifies signs of certain types.
