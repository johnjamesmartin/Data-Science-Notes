## 21. Ensemble Learning

Ensemble learning uses multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms. Random forests (using multiple decision trees and bagging (bootstrap aggregating)) is an example of this.

In other words, ensemble learning is multiple models trying to solve the same problem and with a vote on the results.

### 21.1. Computation

In ensemble learning, many models are built by training on randomly-drawn subsets of data. There are various terms associated with this process. For example, "boosting" which that addresseses data mis-classified by the previous model, a "bucket of models" which is training several different models using training data and picking the one that works best with test data, and "stacking" which runs multiple models at once on the data and combines results together.

### 21.2. Advanced Ensemble Learning

Advanced ensemble learning can be effective but with weak points (sometimes it's best to keep it simple). All the same, here are some advanced approaches:

- Bayes Optimal Classifier. This is theoretically the best, but almost always impractical

- Bayesian Parameter Averaging. This attempts to make BOC practical,Â but is still misunderstood, susceptible to overfitting, and often outperformed in simpler bagging approaches

- Bayesian Model Combination. This tries to address all of those problems, but in the end it's about the same as using cross-validation to find the combination of models
