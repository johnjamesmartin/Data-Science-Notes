## 21. Ensemble Learning

Ensemble learning uses multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms. Random forests (using multiple decision trees and bagging (bootstrap aggregating)) is an example of this.

In other words, ensemble learning just means we use multiple models to try and solve the same problem and let them vote on the results.

### 21.1. Computation

In ensemble learning, many models are built by training on randomly-drawn subsets of data. There are various terms associating with this process. For example, "boosting" which that addresseses data mis-classified by the previous model, a "bucket of models" which is training several different models using training data and picking the one that works best with test data, and "stacking" which runs multiple models at once on the data and combines results together.

### 21.2. Advanced Ensemble Learning

Advanced ensemble learning can be effective but with weak points (sometimes it's best to keep it simple).

- Bayes Optimal Classifier. This is theoretically the best, but almost always impractical

- Bayesian Parameter Averaging. This attempts to make BOC practical,Â but is still misunderstood, susceptible to overfitting, and often outperformed in simpler bagging approaches

- Bayesian Model Combination. This tries to address all of those problems, but in the end it's about the same as using cross-validation to find the combination of models
