## 39. History of Artificial Networks

Artificial neural networks has a biological inspiration in that it's largely based on an understanding of how brains work. In a nutshell, this involves neurons in your cerebral cortex connected to one another via axons. These neurons fire to other neurons they are connected to when enough of its input signals are activated. This is a very simple process at the individual neuron level, but layers of neurons connected in this way can yield learning behaviour with billions of neurons, each with thousands of connections yielding a mind.

### 39.1. Cortical columns

Neurons in a brain's cortex seem to be arranged into many stacks of columns that process information in parallel. "Mini columns" of around 100 neurons are organised into larger "hyper columns". There are 100 million mini-columns in the cortex. This, coincidentally, is analogous to GPUs and how they work.

### 39.2. The First Artificial Neurons

Work on artificial neurons goes all the way back to 1943. We understand artificial neurons in that they fire if more than N input connections are active. Depending on the number of connections from each input neuron and whether a connection activates or suppresses a neuron, you can construct AND, OR, and NOT logical constructs (boolean) this way.

### 39.3. The Linear Threshold Unit (LTU)

LTUs as a concept first emerged around 1957. It adds weights (rather than just binary on and off switches) to inputs; output is given by a step function.

Essentially: sum up the products of inputs and their weights. Output 1 if sum is >= 1.

### 39.4. The Perceptron

The perceptron comprises a layer of LTUs. It can learn by reinforcing weights that lead to correct behaviour during training. Like the previously discussed concepts, the perceptron too has a biological basis ("cells that fire together, wire together").

### 39.5. Multi-Layer Perceptron

Multi-layer perceptrons introduces "hidden layers". This is a deep neural network. Training is trickier.

### 39.6. A Modern Deep Neural Network

A modern deep neural network replaces the step activiation function with something better.
The likes of "softmax" can be applied to output and training uses gradient descent.
