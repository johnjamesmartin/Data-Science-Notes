## 36. A/B Testing Concepts

An A/B test is a controlled experiment — usually in the context of a website. You test the performance of some change to your website (the variant) and measure conversation relative to you unchanged site (the control).

### 36.1. Purpose

You can test almost anything with A/B testing, including:

- Design changes

- UI flow

- Algorithmic changes

- Pricing changes

### 36.2. Measuring Conversion

Ideally choose what you are trying to influence:

- Order amounts

- Profit

- Ad clicks

- Order quantity

However, attributing actions downstream from you change can be hard — especially if you're running more than one experiment.

### 36.3. Variance is the Enemy

A common mistake is to run a test for some small period of time that results in a few purchases to analyse. To do this, you might take the mean order amount from A and B and declare victory or defeat. However, there's so much random variation in order amounts to begin with that your result could be based on chance.

This caveat is important to bear in mind because failing for it can be harmful and cost the company a lot of money.

### 36.4. Further Caveats

- Correlation does not imply causation. Even a low P value score on a well-designed experiment does not imply causation. There could still be random chance or other factors at play. It is the duty of a data scientist to ensure business owners understand this.

- Novelty effects. Changes to a website will catch the attention of previous users who are used to the way it used to be and they might click on something simply because it's new — a temporary uptick in attention. A good idea is to re-run experiments much later and validate this impact, because often the old website will outperform the new one after a while simply because it is a change.

- Seasonal effects is another "gotcha" with A/B testing in that consumer behaviour durings holidays like Christmas will be very different from other times of the year.

- Selection bias. Sometimes your random selection of customers for A or B isn't really random. For example, assignment based on a customer ID, but customers with low IDs are better customers than ones with high IDs. The solution is to run an A/A test periodically to check and auditing your segment assignment algorithms.

- Data pollution. Are robots (both self-identified and malicious) affecting your experiment? More generally are outliers skewing the result?

- Attribution errors. Often there are errors in how conversion is attributed to an experiment. Using a widely used A/B test platform can help mitigate that risk. Watch for "grey areas" — are you counting purchases toward an experiment within some given time-frame of exposure to it? Is that time too large? Could other changes downstream from the change you're measuing affect your results? Are you running multiple experiments at once?
