## 18. Measuring Entropy

Entropy is a measure of a data set's disorder — how similar or different it is.

If we classify a data set into N different classes, e.g. animal attributes vs. species:

- The entropy is 0 if all the classes in the data are the same (everyone is an iguana)

- The entropy is high if they're all different
