## 29. K-Fold Cross Validation

K-Fold cross validation is a way to protect against overfitting. It sounds complex but, in reality, it's quite simple:

1. Split data into K randomly-assigned segments

2. Reserve one segment as your test data

3. Train on each of the remaining K-1 segments and measure their performance against the test set

4. Take the average of the K-1 r-squared scores

This prevents us overfitting to a single trian/test split.

### 29.1. Using K-Fold Cross Validation

Scikit-Learn makes it very easy â€” even easier than a single train/test split. In practice, we need to try different variations of the model and measure the mean accuracy using K-fold cross validation until a "sweet spot" is found (that which reduces errors the most).

See `KFoldCrossValidation.ipynb` for more.
