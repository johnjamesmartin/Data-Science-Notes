## 20. Decision Trees

Decision trees are perhaps the most interesting aspect of machine learning. It's a form of supervised learning in which we construct a flowchart to help us decide a classification for something.

### 20.1. Computation

Essentially, we insert sample data and resulting classifications and out comes a "tree".

We can train a decision tree on data to make predictions. At each step, find the attribute we can use to partition the data set to minimize entropy at the next step. The fancy term for this simple algorithm: ID3

### 20.2. Caveats

It is a greedy algorithm — as it goes down the tree, it picks the decision that redcues entropy the most at that stage

Decision trees are susceptible to overfitting, so be careful! To reduce this, we can construct several alternate decision trees (a forest). We then randomly resample input data from each, i.e. "bootstrap aggregating" or "bagging".

See `DecisionTree.ipynb` for more.
