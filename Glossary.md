## Glossary

Artificial intelligence (AI) - the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, and self-correction.

Bagging - stands for bootstrap aggregating and is an ensemble learning technique. Is used when our goal is to reduce variance of a decision tree (reducing overfitting and increasing accuracy) by taking a number of decision tree predictions and averaging them.

Bell curve - the most common type of distribution for a variable, AKA. normal distribution. It is called a "bell curve" because of the bell-shaped line this forms on a graph.

Bin -

Boosting- is an ensemble learning technique to create a collection of predictors.

Bucket -

Categorical data -

Continuous data -

Coefficient -

Data munging/wraggling -

Data science -

Discrete data -

Exponential -

Feature - is a machine learning term that describes an individual measurable property of characteristic of something be observed. In "computer vision" technology this might be an edge or an object.

Group/grouping -

Kertosis -

Linear regression -

Machine learning - an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed.

Mean (mu) -

Median -

Mode -

Neural network - a series of algorithms that calculate underlying relationships in a datasetthrough a process that mimics human brain processes. It adapts to changing inputs without needing to redesign output criteria.

Ordinal data -

Random distribution -

Skewness -

Stacking - is an ensemble learning technique that combines multiple classification or regression models. The base level models are trained based on a complete training set and the meta-model is trained on outputs of the base level model as features (see feature).

Variance
