## 25. Dimensionality Reduction; Principle Component Analysis

The world is full complexity that when quantitified into data sets is going have us dealing with many dimensions. Ideally, these need to be reduced in order to more easily think about and visualise.

### 25.1. Curse of dimensionality

When people talk about the "curse of dimensionality" they're talking about principle component analysis.

Many problems can be thought of as having a huge number of dimensions, for example: recommending movies where the ratings vector may represent a dimension and every movie is its own dimension. This is complicated. However, dimensionality reduction attempts to distill higher-dimensional data down to a smaller number of dimensions while preserving as much of the variance in data as possible.

K-means clustering is an example of a dimensionality reduction algorithm. It reduces data down to K dimensions.

### 25.2. Principal Component Analysis

Principal Component Analysis (PCA) is normally what people are talking about when discussing dimensionality reduction. It involves fancy maths, but at a high level. Basically, it finds "eigenvectors" in the higher dimensional data. These define hyperplanes that split the data while preserving the most variance in it. The data is then projected onto these hyperplanes which represent lower dimensions we wish to represent. A popular implementation of this is called Singular Value Decomposition (SVD).

### 25.3. Purpose

Principal Component Analysis is useful for things like image compression and feature extraction applications like facial recognition.

In the Python package SciKit-Learn, there's an in-built data set that includes 4D visualisation data on Iris flowers with values for petals and sepals with lengths and width for each across various Iris specimens. It also includes the subspecies classification of each flower. And what PCA lets us do with this is visualize it all in two dimensions instead of four — while preserving variance.

See `PCA.ipynb` for more.
