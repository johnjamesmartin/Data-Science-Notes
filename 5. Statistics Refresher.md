## 5. Statistics 101 Refresher: Mean, Median, & Mode

Most of the work of a data scientist requires knowledge of the statistics. As such, let's quickly recap "mean", "median", and "mode" — a topic covered in primary school these days, but fundamental to statistics:

### 5.1. Mean

"Mean" is also known as "the average". It is calculated by sum of values in data set divided by number of values in data set.

### 5.2. Median

"Median" is calculated by sorting values and taking the value at midpoint. If there happens to be an even number of values, we calculate the median by taking an average of two midpoint values.

Medians are generally less suspectible to outliers than means.
Imagine the UK had an average household income of £45,000. A small handful of billionaires could skew this average so that the government can make it sounds that a typical household is doing much better than they actually are — the median might only be £40,000. This is a common way for people to lie to you with statistics.

### 5.3. Mode

"Mode" is the most common value in a data set. It is therefore only relevant to discrete numerical data sets. For example, if we have a list of children per household: 0, 2, 3, 2, 1, 0, 0, 2, 0 . This list contains four 0s (0:4), one 1 (1:1), three 2s (2:3), and one 3 (3:1). As such, the mode is 0 children per household, because there's more 0 instances than any other value

### 5.4. Using Mean, Median, & Mode in Python

We can use the NumPy library in Python to create a random distribution on a bell curve around a certain point — perhaps a mean. We would do this like so:

```
import numpy as np

incomes = np.random.normal(27000, 15000, 10000)
np.mean(incomes)
```

In this case that point is 27000 (income amount) with a standard deviation of 15000 with 10000 data points (10000 workers) in the data set.

We can use the package MatPlotLib to create a histogram of the results with income data quantized into 50 "buckets" (AKA. "groups"/"discrete variables"/"bins"/etc.):

```
%matplotlib inline
import matplotlib.pyplot as plt
plt.hist(incomes, 50)
plt.show()
```

Although it has been pointed out previously, it's very important to keep in mind that outliers can skew the mean. For example, if Jeff Bezo was in this data set (US incomes) and making a billion dollars a year, the mean income would be well over 100,000 whereas the median would remain around the same. Again, using mean is something to be wary of as it's a common trick — lying to people with statistics.

### 5.5. Standard Deviation and Variance

Standard deviation and variance are two fundamental quantities of data distribution that we see a lot in data science. For simplicity, let's look at variance first:

#### 5.5.1. Variance

Variance (s² or σ² (sigma squared)) is how spread out a data set is. It is calculated by taking the average of square differences from the mean. For example, let's calculate the variance of this list: [1, 4, 5, 4, 8]:

1. Mean is calculated with 1 + 4 + 5 + 4 + 8 / 5, which is 4.4

2. The differences from the mean are -3.4, -0.4, 0.6, -0.4, 3.6

3. The squared differences are 11.56, 0.16, 0.36, 0.16, 12.96

4. Averaging these squared differences 11.56 + 0.16 + 0.36 + 0.16 + 12.96 = 5.04

5. In conclusion, the variance (s² or σ² (sigma squared)) is 5.04

#### 5.5.2. Standard Deviation

Standard deviation is used to identify outliers in a data set. Data scientists sometimes talk about how many standard deviations something is from the mean. Essentially, it is the root of the variance √σ² ( because sigma (σ) represents standard deviation).

Population (σ²) vs. sample variance (s²) represent different variance types. Remember that if you're working with a complete set of data — a complete observation — you do the variance as normal (described above). However, if you're **sampling** the data, you divide by the number of samples _minus one_

![Standard Deviation and Variance Equations](https://i.imgur.com/s1UsKeu.gif 'Equations')

#### 5.5.3. Standard Deviation in Python

Using NumPy we can calculate standard deviation very easily with the .std() method:

```
import numpy as np
import matplotlib.pyplot as plt

# normal method takes three arguments: mean (mu), standard deviation (sigma), datapoints:
incomes = np.random.normal(100.0, 20.0, 10000)

plt.hist(incomes, 50)
plt.show()

incomes.std()
```

#### 5.5.4. Standard Deviation and Variance Notes in Jupyter Notebook

For more refresher notes, see `StdDevVariance.ipynb` under Jupyter notebook

### 5.6. Probability Density Function

In continous data sets, any given value has an extremely small chance of occuring. For example, the probability of completing a task in exactly 9.200009393383 or 41.3939399922 seconds is vanishingly small.

What probability density functions allow us to do is speak to the probability of a data point falling within some given range of values, which is much more meaningful and useful. For example, a value falling within a standard deviation could be 34.1%.

### 5.7. Probability Mass Function

In discrete data sets, we can draw a curve on a histogram and get the probability from where it intersects on x and y without having to deal with ranges.

### 5.8. Common Data Distributions

1. Uniform distribution. This has equal probability across any given value within the range defined. We can use NumPy to achieve this quite easily with uniform() method:

   ````
   import numpy as np
   import matplotlob.pyplot as plt

   values = np.random.uniform(-10.0, 10.0, 10000)
   plt.hist(values, 50)
   plt.show()
   ```
   ````

2. Normal distribution AKA. gaussian distribution. This is symmetric about the mean (mu) such that it appears like a bell curve. Uses the probability density function

3. Exponential PDF / "Power Law". This is where values start high but fall off quickly in an exponential manner (downward curve). You see this happening a lot in nature

4. Binomial (Probability Mass Function). Probability mass functions deal with discrete data. Binomial deals with n repeated trials with each resulting in two possible outcomes: one a success (denoted by P and the same every trial) and one a failure

5. Poisson (Probability Mass Function). If you have information about the average number of things happening in a given time period, this gives you a way of predicting some other values instead in the future. Example: I have a website with 500 views on average a day and can use Poisson Probability Mass Function to work out the odds of seeing 550 views on a specific day.

### 5.8.1. Pop Quiz

_What's the equivalent of a probability distribution function when using discrete instead of continous data?_

Answer:

Probability mass function

### 5.9. Percentiles & Moments

#### 5.9.1. Percentile

A percentile is the value below which a percentage of data falls. The top 1% would be the 99th percentile. The 50th percentile would be the median on normal distribution. Quartiles are points at the median that take up 50% of data (25% each). They're grouped in a "box & whisker" diagram as an IQR (interquartile range). NumPy has a .percentile() method. See `Percentiles.ipynb` for more.

#### 5.9.2. Moments

Moments are quantitative measures of the shape of probability density functions. Let's look at four moments:

1. First moment = the mean

2. Second moment = the variance

3. Third moment = the skew. The skew is how lop-sided the distribution is (tail skewed left is a negative skew). See https://en.wikipedia.org/wiki/Skewness for more. For a skew function, we need to import "scipy.stats"

4. Fourth moment = the kurtosis. Kurtosis is how sharp the peak is compared to normal distribution. Higher peaks have a higher kurtosis. For a kurtosis function, we need to import "scipy.stats"
