## 34. TF-IDF

TF-IDF stands for "Term Frequency" and "Inverse Document Frequency". It deals with important data for a search — figuring out which terms are most revelant for a document.

### 34.1. Term Frequency

Term frequency deals with measuring how often a word occurs in a document. A word that occurs frequently is probably important to the document's meaning.

### 34.2. Document Frequency

Document frequency deals with measuring how often a word occurs in an entire set of documents, i.e. all of Wikipedia or every web page. This includes _all_ words — even "a", "the', "and", etc.

### 34.3. TF-IDF Measurement

A TF-IDF is the measure of a words relevancy; expressed as `Term Frequency / Document Frequency` or `Term Frequency * Inverse Document Frequency`. This is: take how often the word appears in a document over how often it just appears everywhere. This gives you a measure of how important and unique this word is for this document.

### 34.4. TF-IDF In Practice

We actually use the log of the IDF (since word frequencies are distributed exponentially). This gives us a better weighting of a words overall popularity.

There are some limitations with TF-IDF. For example, it assumes a document is just a "bag of words" with no links formed between words that are synonyms or words in different formats and so on. Plus, this, the parsing of documents into a "bag of words", is often most of the work. However, we can increase efficiency: words can be represented as a hash value.

Doing all of this at scale is difficult. This is where technologies like Spark helps us.

### 34.5. TF-IDF Purpose

TF-IDFs are used in search engines. A way of building our own simple search algorithm might be: include computing the TF-IDF for every word in a corpus. Then for a given search word, sort the documents by their TF-IDF score for that word and display the overall results.
